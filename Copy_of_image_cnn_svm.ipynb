{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "connect google colab with google drive"
      ],
      "metadata": {
        "id": "zAtyab7xxa_d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgVOPMFH-rdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c969bdd9-7255-4103-e224-8404d0d7ad5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#connect google colab with google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries to load dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "#opens file in readmode\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/MajorProject/br35hdataset/train.zip')\n",
        "\n",
        "#extracts the files into the /MajorProject folder\n",
        "zip_ref.extractall('/content/gdrive/MyDrive/MajorProject/br35hdataset')\n",
        "\n",
        "#close the file\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "u6i5i4GCnCZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries to load dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "#opens file in readmode\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/MajorProject/br35hdataset/validation.zip')\n",
        "\n",
        "#extracts the files into the /MajorProject folder\n",
        "zip_ref.extractall('/content/gdrive/MyDrive/MajorProject/br35hdataset')\n",
        "\n",
        "#close the file\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "13t41rtVnRnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the libraries to load dataset\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "#opens file in readmode\n",
        "zip_ref = zipfile.ZipFile('/content/gdrive/MyDrive/MajorProject/br35hdataset/test.zip')\n",
        "\n",
        "#extracts the files into the /MajorProject folder\n",
        "zip_ref.extractall('/content/gdrive/MyDrive/MajorProject/br35hdataset')\n",
        "\n",
        "#close the file\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "e8xZK5-tnUez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.metrics import f1_score\n",
        "#from sklearn.utils import shuffle\n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import os \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "mNMIqf1bHC4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing variables\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS=3\n",
        "EPOCHS=30\n",
        "num_classes=2"
      ],
      "metadata": {
        "id": "O4VRYxPPIHmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "rO7JhvqaBR-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess image\n",
        "def preprocess_image(image_path):\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    # Blur image\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    # Threshold image to binary\n",
        "    _, threshold_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "    # Erosion and Dilation for noise removal\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "    opening = cv2.morphologyEx(threshold_image, cv2.MORPH_OPEN, kernel)\n",
        "    # Find contours and select the largest contour\n",
        "    contours, _ = cv2.findContours(opening, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    # Calculate extreme points of the contour\n",
        "    leftmost = tuple(largest_contour[largest_contour[:,:,0].argmin()][0])\n",
        "    rightmost = tuple(largest_contour[largest_contour[:,:,0].argmax()][0])\n",
        "    topmost = tuple(largest_contour[largest_contour[:,:,1].argmin()][0])\n",
        "    bottommost = tuple(largest_contour[largest_contour[:,:,1].argmax()][0])\n",
        "    # Crop image using contour and extreme points\n",
        "    cropped_image = image[topmost[1]:bottommost[1], leftmost[0]:rightmost[0]]\n",
        "    # Resize cropped image using bicubic interpolation\n",
        "    resized_image = cv2.resize(cropped_image, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "    # Return preprocessed image\n",
        "    return resized_image"
      ],
      "metadata": {
        "id": "4buEKYE8O5ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read images from directory and preprocess them\n",
        "def preprocess_images(directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for subdirectory in os.listdir(directory):\n",
        "        subdirectory_path = os.path.join(directory, subdirectory)\n",
        "        if os.path.isdir(subdirectory_path):\n",
        "            for image_filename in os.listdir(subdirectory_path):\n",
        "                image_path = os.path.join(subdirectory_path, image_filename)\n",
        "                # Preprocess image and append to list\n",
        "                preprocessed_image = preprocess_image(image_path)\n",
        "                images.append(preprocessed_image)\n",
        "                # Append label based on subdirectory name\n",
        "                if subdirectory == 'yes':\n",
        "                    labels.append(1)\n",
        "                else:\n",
        "                    labels.append(0)\n",
        "    # Convert images and labels to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    # Return preprocessed images and labels\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "Kgh3YIjoFaXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = '/content/gdrive/MyDrive/MajorProject/br35hdataset/train'\n",
        "validation_directory = '/content/gdrive/MyDrive/MajorProject/br35hdataset/validation'\n",
        "test_directory = '/content/gdrive/MyDrive/MajorProject/br35hdataset/test'\n",
        "\n",
        "train_images, train_labels = preprocess_images(train_directory)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/train_images.npy', train_images)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/train_labels.npy', train_labels)\n"
      ],
      "metadata": {
        "id": "MURWy4hrUBX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the images and labels to files\n",
        "test_images, test_labels = preprocess_images(test_directory)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/test_images.npy', test_images)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/test_labels.npy', test_labels)\n",
        "\n",
        "validation_images, validation_labels = preprocess_images(validation_directory)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/validation_images.npy', validation_images)\n",
        "np.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/validation_labels.npy', validation_labels)"
      ],
      "metadata": {
        "id": "mYZ0qhxnGWD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, kernel_size = (3,3), activation='relu',input_shape=(IMAGE_SIZE, IMAGE_SIZE,CHANNELS)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(rate=0.2),\n",
        "    layers.Conv2D(64, kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(rate=0.2),\n",
        "    layers.Conv2D(128, kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(rate=0.2),\n",
        "    layers.Conv2D(128, kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(rate=0.2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(rate=0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "exlc_t7HVJco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "ujR3eYbX3QKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkZEuDpy6eGC",
        "outputId": "32cb40a6-98b1-4130-fb1e-107f80c5d227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 127, 127, 32)      0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 62, 62, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               3211392   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,452,353\n",
            "Trainable params: 3,452,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# define the path and filename to save the model weights\n",
        "filepath = \"/content/gdrive/MyDrive/MajorProject/br35hdataset/weights_cnn_svm.h5\"\n",
        "\n",
        "# create a ModelCheckpoint callback to save the best model weights during training\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "MWB1E4UtWmv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model on preprocessed images and labels\n",
        "model.fit(train_images, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(validation_images, validation_labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "0YyqUDoO0qqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e7a8b45-5712-438f-ace1-25775704bd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "450/450 [==============================] - 75s 148ms/step - loss: 5.3821 - accuracy: 0.7247 - val_loss: 0.5136 - val_accuracy: 0.7667\n",
            "Epoch 2/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.4471 - accuracy: 0.7842 - val_loss: 0.3746 - val_accuracy: 0.8306\n",
            "Epoch 3/30\n",
            "450/450 [==============================] - 65s 145ms/step - loss: 0.3767 - accuracy: 0.8314 - val_loss: 0.3086 - val_accuracy: 0.8689\n",
            "Epoch 4/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.3069 - accuracy: 0.8656 - val_loss: 0.2777 - val_accuracy: 0.8789\n",
            "Epoch 5/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.2642 - accuracy: 0.8878 - val_loss: 0.2468 - val_accuracy: 0.9011\n",
            "Epoch 6/30\n",
            "450/450 [==============================] - 65s 146ms/step - loss: 0.2311 - accuracy: 0.9062 - val_loss: 0.2119 - val_accuracy: 0.9106\n",
            "Epoch 7/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.1892 - accuracy: 0.9274 - val_loss: 0.1733 - val_accuracy: 0.9383\n",
            "Epoch 8/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.1717 - accuracy: 0.9348 - val_loss: 0.2015 - val_accuracy: 0.9233\n",
            "Epoch 9/30\n",
            "450/450 [==============================] - 65s 145ms/step - loss: 0.1449 - accuracy: 0.9469 - val_loss: 0.1606 - val_accuracy: 0.9500\n",
            "Epoch 10/30\n",
            "450/450 [==============================] - 66s 146ms/step - loss: 0.1390 - accuracy: 0.9474 - val_loss: 0.1628 - val_accuracy: 0.9450\n",
            "Epoch 11/30\n",
            "450/450 [==============================] - 65s 144ms/step - loss: 0.1311 - accuracy: 0.9524 - val_loss: 0.1458 - val_accuracy: 0.9489\n",
            "Epoch 12/30\n",
            "450/450 [==============================] - 65s 144ms/step - loss: 0.1130 - accuracy: 0.9573 - val_loss: 0.1339 - val_accuracy: 0.9533\n",
            "Epoch 13/30\n",
            "450/450 [==============================] - 64s 143ms/step - loss: 0.1079 - accuracy: 0.9623 - val_loss: 0.1457 - val_accuracy: 0.9544\n",
            "Epoch 14/30\n",
            "450/450 [==============================] - 64s 143ms/step - loss: 0.1032 - accuracy: 0.9653 - val_loss: 0.1715 - val_accuracy: 0.9467\n",
            "Epoch 15/30\n",
            "450/450 [==============================] - 64s 143ms/step - loss: 0.0969 - accuracy: 0.9631 - val_loss: 0.1428 - val_accuracy: 0.9517\n",
            "Epoch 16/30\n",
            "450/450 [==============================] - 64s 143ms/step - loss: 0.0932 - accuracy: 0.9662 - val_loss: 0.1217 - val_accuracy: 0.9644\n",
            "Epoch 17/30\n",
            "450/450 [==============================] - 64s 141ms/step - loss: 0.0848 - accuracy: 0.9694 - val_loss: 0.1620 - val_accuracy: 0.9439\n",
            "Epoch 18/30\n",
            "450/450 [==============================] - 63s 141ms/step - loss: 0.0862 - accuracy: 0.9686 - val_loss: 0.1075 - val_accuracy: 0.9700\n",
            "Epoch 19/30\n",
            "450/450 [==============================] - 63s 140ms/step - loss: 0.0880 - accuracy: 0.9676 - val_loss: 0.1630 - val_accuracy: 0.9444\n",
            "Epoch 20/30\n",
            "450/450 [==============================] - 62s 138ms/step - loss: 0.0890 - accuracy: 0.9693 - val_loss: 0.1284 - val_accuracy: 0.9628\n",
            "Epoch 21/30\n",
            "450/450 [==============================] - 63s 139ms/step - loss: 0.0795 - accuracy: 0.9726 - val_loss: 0.1239 - val_accuracy: 0.9617\n",
            "Epoch 22/30\n",
            "450/450 [==============================] - 62s 138ms/step - loss: 0.0753 - accuracy: 0.9738 - val_loss: 0.1629 - val_accuracy: 0.9600\n",
            "Epoch 23/30\n",
            "450/450 [==============================] - 61s 137ms/step - loss: 0.0700 - accuracy: 0.9772 - val_loss: 0.1210 - val_accuracy: 0.9661\n",
            "Epoch 24/30\n",
            "450/450 [==============================] - 62s 137ms/step - loss: 0.0696 - accuracy: 0.9758 - val_loss: 0.1627 - val_accuracy: 0.9528\n",
            "Epoch 25/30\n",
            "450/450 [==============================] - 60s 134ms/step - loss: 0.0836 - accuracy: 0.9718 - val_loss: 0.1293 - val_accuracy: 0.9611\n",
            "Epoch 26/30\n",
            "450/450 [==============================] - 60s 134ms/step - loss: 0.0644 - accuracy: 0.9784 - val_loss: 0.1590 - val_accuracy: 0.9544\n",
            "Epoch 27/30\n",
            "450/450 [==============================] - 60s 133ms/step - loss: 0.0739 - accuracy: 0.9751 - val_loss: 0.1127 - val_accuracy: 0.9672\n",
            "Epoch 28/30\n",
            "450/450 [==============================] - 60s 133ms/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.1093 - val_accuracy: 0.9750\n",
            "Epoch 29/30\n",
            "450/450 [==============================] - 60s 134ms/step - loss: 0.0685 - accuracy: 0.9785 - val_loss: 0.1257 - val_accuracy: 0.9628\n",
            "Epoch 30/30\n",
            "450/450 [==============================] - 60s 133ms/step - loss: 0.0582 - accuracy: 0.9804 - val_loss: 0.1179 - val_accuracy: 0.9694\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44ea691610>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the CNN model"
      ],
      "metadata": {
        "id": "-tevH1Vv6gHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save('/content/gdrive/MyDrive/MajorProject/br35hdataset/imagepreprocess_cnn.h5')"
      ],
      "metadata": {
        "id": "c2leFRFHQxFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model on the test dataset"
      ],
      "metadata": {
        "id": "yJMm729R6jzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "hWT7rKlwH9R3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8ebb57-b926-40cf-c918-9cc2509544a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 2s 42ms/step - loss: 0.1400 - accuracy: 0.9639\n",
            "Test accuracy: 0.9638888835906982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved model"
      ],
      "metadata": {
        "id": "3Kg47NuL6dvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load saved CNN model\n",
        "cnn_model = load_model('/content/gdrive/MyDrive/MajorProject/br35hdataset/imagepreprocess_cnn.h5')"
      ],
      "metadata": {
        "id": "ZeiqOH_FxAdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the saved images and labels\n",
        "train_images = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/train_images.npy')\n",
        "train_labels = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/train_labels.npy')\n",
        "test_images = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/test_images.npy')\n",
        "test_labels = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/test_labels.npy')\n",
        "validation_images = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/validation_images.npy')\n",
        "validation_labels = np.load('/content/gdrive/MyDrive/MajorProject/br35hdataset/validation_labels.npy')\n"
      ],
      "metadata": {
        "id": "jmbLi5viJUQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Load the test folder\n",
        "test_data_dir = '/content/gdrive/MyDrive/MajorProject/br35hdataset/test'\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jDxZuwwVEz1",
        "outputId": "0b39c664-229d-48ac-94c8-2736445762e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1800 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate predictions for the test data\n",
        "y_pred = cnn_model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get the true labels for the test data\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Get the class labels from the data generator\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, \n",
        "            yticklabels=class_labels)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "BfvhxcQwRisQ",
        "outputId": "7947d00b-c35d-4bad-9825-c84685518d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 249s 4s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Classifier"
      ],
      "metadata": {
        "id": "IsvcQMFg6s3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# Extract features using CNN model\n",
        "train_features = cnn_model.predict(train_images)\n",
        "test_features = cnn_model.predict(test_images)\n",
        "validation_features = cnn_model.predict(validation_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "3WR3RFZUAX1t",
        "outputId": "67d457cc-1520-4153-cf72-3e62619279cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251/450 [===============>..............] - ETA: 5:16"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f625d9b7e3c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract features using CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2348\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2350\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2351\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2352\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Initialize empty lists to store training and validation accuracies\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "# Train SVM classifier using CNN features\n",
        "clf = svm.SVC()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Shuffle the training features and labels before each epoch\n",
        "    train_features, train_labels = shuffle(train_features, train_labels)\n",
        "\n",
        "    # Train the SVM classifier on the shuffled data\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    # Calculate the training and validation accuracies\n",
        "    train_acc = clf.score(train_features, train_labels)\n",
        "    val_acc = clf.score(validation_features, validation_labels)\n",
        "\n",
        "    # Append the accuracies to the lists\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Print epoch number and accuracy values\n",
        "    print(f\"Epoch {epoch+1}: Train acc = {train_acc:.4f}, Val acc = {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgpZRFyNzmKK",
        "outputId": "520391cd-916f-49e3-8b04-d5907c13340e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 2: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 3: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 4: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 5: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 6: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 7: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 8: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 9: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 10: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 11: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 12: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 13: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 14: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 15: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 16: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 17: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 18: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 19: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 20: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 21: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 22: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 23: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 24: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 25: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 26: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 27: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 28: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 29: Train acc = 0.9981, Val acc = 0.9700\n",
            "Epoch 30: Train acc = 0.9981, Val acc = 0.9700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save SVM classifier\n",
        "with open('/content/gdrive/MyDrive/MajorProject/br35hdataset/svm_model_1.pkl', 'wb') as f:\n",
        "    pickle.dump(clf, f)"
      ],
      "metadata": {
        "id": "jWbSnueC2tOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Load SVM classifier\n",
        "with open('/content/gdrive/MyDrive/MajorProject/br35hdataset/svm_model_1.pkl', 'rb') as f:\n",
        "    svm_classifier = pickle.load(f)"
      ],
      "metadata": {
        "id": "2ISGtHwU0Hfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import svm\n",
        "\n",
        "classes = ['no', 'yes']\n",
        "\n",
        "# Predict the labels for the test data\n",
        "pred_labels = clf.predict(test_images)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(test_labels, pred_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('Confusion matrix:')\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "VAu5WCbu0IXF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "219eef96-1c7e-4fe3-8d98-cc83de30e11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6cd8972bd3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict the labels for the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Compute the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-T8KoFe14H7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}